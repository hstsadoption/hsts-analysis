{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSTS Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import subprocess\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import collections\n",
    "import time\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "import getopt\n",
    "import logging\n",
    "\n",
    "print(\"Versions: Python {}, Pandas {}\"\n",
    "      .format(sys.version, pd.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_VERSION_MAJOR = 2\n",
    "SCRIPT_VERSION_MINOR = 20180709\n",
    "DEBUG = False\n",
    "CLEANUP = False\n",
    "\n",
    "HOSTS_FILE = 'http.csv'\n",
    "META_FILE = 'meta.json'\n",
    "RESULTS_FILE = 'results.json'\n",
    "\n",
    "TMP_DATA_ROOT = '../data/tmp'\n",
    "DATA_SETS = [\n",
    "    {\n",
    "        'id': 'ipv4',\n",
    "        'ipVersion': 4,\n",
    "        'src': '../data/ipv4/' + HOSTS_FILE\n",
    "    }, {\n",
    "        'id': 'ipv6',\n",
    "        'ipVersion': 6,\n",
    "        'src': '../data/ipv6/' + HOSTS_FILE\n",
    "    }\n",
    "]\n",
    "CHUNK_SIZE = 8000000\n",
    "\n",
    "EXT_PREPARED = '.cut_sort'\n",
    "EXT_CHUNK = '.chunk-'\n",
    "EXT_PICKLE = '.pickle'\n",
    "EXT_PARSED = '.parsed'\n",
    "EXT_INCONS_MARKED = '.incons-marked'\n",
    "EXT_REDUCED = '.reduced'\n",
    "\n",
    "EXT_INCONSISTENT = '.inconsistent'\n",
    "EXT_INCONS_EXISTENCE = '.exist'\n",
    "EXT_INCONS_CONFIGURATION = '.config'\n",
    "EXT_INCONS_V4V6 = '.v4v6'\n",
    "\n",
    "maxAgeAggregation = {\n",
    "    'off':       0,\n",
    "    'test':      60,\n",
    "    'day':       60 * 60 * 24,\n",
    "    'week':      60 * 60 * 24 * 7,\n",
    "    'w-hy':      60 * 60 * 24 * 180,\n",
    "    'half-year': 60 * 60 * 24 * 190,\n",
    "    'hy-y':      60 * 60 * 24 * 360,\n",
    "    'year':      60 * 60 * 24 * 370,\n",
    "    'other':     float('Inf')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current process for memory consumption\n",
    "proc = psutil.Process(os.getpid())\n",
    "memInfo = {\n",
    "    'last': 0,\n",
    "    'max': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not execute this when in Jupyter Notebook!\n",
    "def parseCliArguments():\n",
    "    \"\"\"Parses the command line arguments\"\"\"\n",
    "    \n",
    "    global DATA_SETS\n",
    "    global CHUNK_SIZE\n",
    "    global DEBUG\n",
    "    global TMP_DATA_ROOT\n",
    "    \n",
    "    try:\n",
    "        optlist, args = getopt.getopt(sys.argv[1:], 'dhc:r:t:', ['debug', 'help', 'chunk-size=', 'data-root=',\n",
    "                                                               'tmp-root='])\n",
    "    except getopt.GetoptError as err:\n",
    "        print(str(err))\n",
    "        sys.exit(2)\n",
    "\n",
    "    if len(args) > 0:\n",
    "        DATA_SETS = [{'id': i, 'src': s, 'ipVersion': int(v)} for s,i,v in [x.split(':') for x in args]]\n",
    "\n",
    "    for opt, arg in optlist:\n",
    "        if opt in ('-c', '--chunk-size'):\n",
    "            chunkSize = -1\n",
    "            if arg.isdigit():\n",
    "                chunkSize = int(arg)\n",
    "            if chunkSize <= 0:\n",
    "                print(\"Chunk size has to be a positive integer, '%s' given\" % arg)\n",
    "                sys.exit(1)\n",
    "            CHUNK_SIZE = chunkSize\n",
    "        elif opt in ('-d', '--debug'):\n",
    "            DEBUG = True\n",
    "        elif opt in ('-h', '--help'):\n",
    "            print(\"Usage: python3 %s [OPTIONS] source(s)\" % sys.argv[0])\n",
    "            print(\"\")\n",
    "            print(\"Source:\")\n",
    "            print(\"<inFile>:<id>:<ipVersion>\")\n",
    "            print(\"\")\n",
    "            print(\"Options:\")\n",
    "            print(\"-c <size>, --chunk-size=<size>  Set the chunk size to <size>\")\n",
    "            print(\"-d, --debug                     Enables debug logging\")\n",
    "            print(\"-h, --help                      Displays this help message\")\n",
    "            print(\"-t <dir>, --tmp-root=<dir>      Sets the temporary directory to <dir>\")\n",
    "            sys.exit()\n",
    "        elif opt in ('-t', '--tmp-root'):\n",
    "            if not os.path.isdir(arg):\n",
    "                print(\"Temporary data root has to be a directory, '%s' given\" % arg)\n",
    "                sys.exit(1)\n",
    "            TMP_DATA_ROOT = arg\n",
    "\n",
    "parseCliArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_LINES_BULK = int(CHUNK_SIZE * 0.0001)\n",
    "MERGE_CHUNK_SIZE = CHUNK_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logger\n",
    "logging.basicConfig(format=\"%(asctime)s [%(levelname)s] at %(funcName)s:%(lineno)d: %(message)s\")\n",
    "logger = logging.getLogger('hsts-analysis')\n",
    "\n",
    "fileHandler = logging.FileHandler(os.path.join(TMP_DATA_ROOT, 'hsts-analysis.log'), mode='w')\n",
    "fileHandler.setFormatter(logging.Formatter(fmt=\"%(asctime)s [%(levelname)s] at %(funcName)s:%(lineno)d: %(message)s\"))\n",
    "\n",
    "logger.addHandler(fileHandler)\n",
    "if DEBUG:\n",
    "    logger.setLevel(\"DEBUG\")\n",
    "else:\n",
    "    logger.setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutAndSort(dataSet):\n",
    "    \"\"\"Selects only relevant columns from a scan file and sorts the rows by hostname afterwards.\"\"\"\n",
    "    \n",
    "    inFile = buildFileName(dataSet, tmp=False)\n",
    "    outFile = buildFileName(dataSet, prepared=True)\n",
    "    subprocess.run(['./cut_sort.sh', inFile, outFile], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sha512sum(file):\n",
    "    \"\"\"Generates the SHA512 sum for the given file.\"\"\"\n",
    "    \n",
    "    sha512 = subprocess.run(['sha512sum', file], check=True, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    sha512 = sha512.split()[0]\n",
    "    logger.debug(\"SHA512 of file '%s': %s\" % (file, sha512))\n",
    "    return sha512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResults(*dataSets):\n",
    "    \"\"\"Saves the results in the data sets.\"\"\"\n",
    "    \n",
    "    for ds in dataSets:\n",
    "        with open(os.path.join(ds['dir'], RESULTS_FILE), 'w') as resultsFile:\n",
    "            json.dump(ds['results'], resultsFile, indent=2, sort_keys=True)\n",
    "    \n",
    "    logger.debug(\"Saved results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveMeta(*dataSets):\n",
    "    \"\"\"Saves the meta data of the data sets.\"\"\"\n",
    "        \n",
    "    for ds in dataSets:\n",
    "        with open(os.path.join(ds['dir'], META_FILE), 'w') as metaFile:\n",
    "            json.dump(ds['meta'], metaFile, indent=2, sort_keys=True)\n",
    "    \n",
    "    logger.debug(\"Saved meta data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMemory():\n",
    "    \"\"\"Prints current memory consumption.\"\"\"\n",
    "    \n",
    "    global proc\n",
    "    global memInfo\n",
    "    \n",
    "    current = proc.memory_info().rss\n",
    "    if current > memInfo['max']:\n",
    "        memInfo['max'] = current\n",
    "    \n",
    "    logger.debug(\"[MEMORY] Current: %.2fMB (%+.3fMB), Max: %.2fMB\" % (\n",
    "        current / 1000 / 1000,\n",
    "        (current - memInfo['last']) / 1000 / 1000,\n",
    "        memInfo['max'] / 1000 / 1000\n",
    "    ))\n",
    "    memInfo['last'] = current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidFile(fileName, fileHash = None):\n",
    "    \"\"\"Checks if the file exists and no zero size. If a hash is given, it will also be checked.\"\"\"\n",
    "    \n",
    "    file = Path(fileName)\n",
    "    # Check file exists\n",
    "    if not (file.exists() and file.is_file() and os.path.getsize(fileName) > 0):\n",
    "        logger.debug(\"File '%s' does not exist!\" % fileName)\n",
    "        return False\n",
    "    # Check hash if given\n",
    "    if fileHash and fileHash != sha512sum(fileName):\n",
    "        logger.debug(\"Hashes don't match for file '%s'!\" % fileName)\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterChunk(df):\n",
    "    \"\"\"Filters only valid data in chunk\"\"\"\n",
    "    \n",
    "    # Drop invalid rows\n",
    "    if not df.empty:\n",
    "        df.dropna(subset=['host', 'server_name', 'http_code'], how='any', inplace=True)\n",
    "        logger.debug(\"Dropped invalid entries (%d remaining)\" % len(df.index))\n",
    "    \n",
    "    # Drop unsuccessful connections\n",
    "    if not df.empty:\n",
    "        df = df[df['http_code'] >= 0]\n",
    "        logger.debug(\"Dropped unsuccessful entries (%d remaining)\" % len(df.index))\n",
    "    \n",
    "    if not df.empty:\n",
    "        df = df[['host', 'server_name', 'headers']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readChunk(ds, nr, start, count = CHUNK_SIZE):\n",
    "    \"\"\"Reads a chunk from the given file, filters the relevant information and saves it as pickle.\n",
    "    Returns the number of read and relevant lines.\"\"\"\n",
    "    \n",
    "    file = buildFileName(ds, prepared=True)\n",
    "    logger.debug(\"Reading chunk %.0f from %s (lines %d - %d)\" % (nr, file, start, start + count))\n",
    "    \n",
    "    # Read csv\n",
    "    colNames = ['host', 'server_name', 'http_code', 'headers']\n",
    "    colTypes = {col: str for col in colNames}\n",
    "    colTypes['http_code'] = float # Cannot be int as NaN != int\n",
    "    df = pd.read_csv(file, skiprows=start, nrows=count, encoding='ISO-8859-1', names=colNames, dtype=colTypes)\n",
    "    linesRead = len(df.index)\n",
    "    printMemory()\n",
    "    \n",
    "    logger.debug(\"Read %d lines\" % linesRead)\n",
    "    \n",
    "    df = filterChunk(df)\n",
    "    linesRelevant = len(df.index)\n",
    "    \n",
    "    if linesRelevant > 0:\n",
    "        # Read additional lines to have all lines of one server_name in one chunk\n",
    "        lastServerName = df.iloc[-1]['server_name']\n",
    "        logger.debug(\"Last server_name: %s\" % lastServerName)\n",
    "        additionalLinesRead = 0\n",
    "        additionalLinesRelevant = 0\n",
    "        \n",
    "        # Reading lines in bulk is faster than reading every line idividually\n",
    "        additionalLinesFound = True\n",
    "        while additionalLinesFound:\n",
    "            logger.debug(\"Looking for additional lines for last host in lines %d to %d\" % (\n",
    "                    start + count + additionalLinesRead, start + count + additionalLinesRead + ADDITIONAL_LINES_BULK\n",
    "                ))\n",
    "            linesDf = pd.read_csv(file, skiprows=start + count + additionalLinesRead,\n",
    "                                  nrows=ADDITIONAL_LINES_BULK, encoding='ISO-8859-1', names=colNames, dtype=colTypes)\n",
    "            printMemory()\n",
    "            \n",
    "            if len(linesDf.index) <= 0:\n",
    "                # No more lines to read\n",
    "                logger.debug(\"EOF reached\")\n",
    "                additionalLinesFound = False\n",
    "                break\n",
    "            \n",
    "            # Counts the additional lines that have been processed in this sub-chunk\n",
    "            linesReadInSubChunk = 0\n",
    "            linesDf = filterChunk(linesDf)\n",
    "            \n",
    "            for i in range(len(linesDf.index)):\n",
    "                lineServerName = linesDf.iloc[i]['server_name']\n",
    "                lineNrInSubChunk = int(linesDf.iloc[i].name)\n",
    "                logger.debug(\"Next server name: %s (@%d)\" % (lineServerName, lineNrInSubChunk))\n",
    "                if lineServerName == lastServerName:\n",
    "                    df = df.append(linesDf.iloc[i], ignore_index=True)\n",
    "                    # Line nrs start at 0\n",
    "                    linesReadInSubChunk = lineNrInSubChunk + 1\n",
    "                    additionalLinesRelevant += 1\n",
    "                else:\n",
    "                    additionalLinesFound = False\n",
    "                    break\n",
    "                    \n",
    "            additionalLinesRead += linesReadInSubChunk\n",
    "\n",
    "            del linesDf\n",
    "            gc.collect()\n",
    "            \n",
    "            printMemory()\n",
    "        \n",
    "        logger.debug(\"Read %d additional lines and found %d relevant ones\" %\n",
    "                     (additionalLinesRead, additionalLinesRelevant))\n",
    "    \n",
    "        linesRead += additionalLinesRead\n",
    "        linesRelevant += additionalLinesRelevant\n",
    "    \n",
    "    logger.debug(\"Read %d lines from csv\" % linesRead)\n",
    "    printMemory()\n",
    "    \n",
    "    if linesRead > 0:\n",
    "        if nr != None:\n",
    "            # Save loaded chunk\n",
    "            df.to_pickle(buildFileName(ds, chunk=nr))\n",
    "            logger.debug(\"Saved chunk\")\n",
    "        \n",
    "    # Free memory\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    printMemory()\n",
    "    \n",
    "    return linesRead, linesRelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildFileName(ds, prepared = False, chunk = np.nan, status = None, tmp = True):\n",
    "    \"\"\"Builds the name of the file.\"\"\"\n",
    "    \n",
    "    if not tmp and not (prepared or chunk == chunk or status):\n",
    "        return ds.get('src')\n",
    "    \n",
    "    file = os.path.join(ds.get('dir'), HOSTS_FILE)\n",
    "    \n",
    "    if prepared or chunk == chunk:\n",
    "        file += EXT_PREPARED\n",
    "    if chunk == chunk:\n",
    "        file += EXT_CHUNK + str(chunk)\n",
    "    if status == 'parsed' or status == 'marked' or status == 'reduced':\n",
    "        file += EXT_PARSED\n",
    "    if status == 'marked' or status == 'reduced':\n",
    "        file += EXT_INCONS_MARKED\n",
    "    if status == 'reduced':\n",
    "        file += EXT_REDUCED\n",
    "    if chunk == chunk:\n",
    "        file+= EXT_PICKLE\n",
    "    \n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadChunk(ds, nr, status = None):\n",
    "    \"\"\"Loads the specified chunk of the given dataset.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        return pd.read_pickle(buildFileName(ds, chunk=nr, status=status))\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(ds):\n",
    "    \"\"\"Loads the meta data and results for the given data set.\"\"\"\n",
    "    \n",
    "    logger.info(\"Building data set %s from source file '%s'\" % (ds['id'], ds['src']))\n",
    "    \n",
    "    # Store temporary files in here\n",
    "    ds['dir'] = os.path.join(TMP_DATA_ROOT, ds['id'])\n",
    "    # Id that can be used in pandas queries\n",
    "    ds['id_normal'] = re.sub(r'^\\d+|[^a-zA-Z_0-9]', '', ds['id'].replace('-', '_'))\n",
    "    \n",
    "    if not os.path.exists(ds['dir']):\n",
    "        os.makedirs(ds['dir'])\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(ds['dir'], META_FILE), 'r') as f:\n",
    "            ds['meta'] = json.loads(f.read())\n",
    "    except FileNotFoundError:\n",
    "        # No meta data exist\n",
    "        \n",
    "        srcFileSize = -1\n",
    "        if isValidFile(ds['src']):\n",
    "            srcFileSize = os.path.getsize(ds['src'])\n",
    "        \n",
    "        ds['meta'] = {\n",
    "            'script': {\n",
    "                'version': {\n",
    "                    'major': SCRIPT_VERSION_MAJOR,\n",
    "                    'minor': SCRIPT_VERSION_MINOR\n",
    "                },\n",
    "                'source': {\n",
    "                    'file': {\n",
    "                        'name': ds['src'],\n",
    "                        'size': srcFileSize\n",
    "                    },\n",
    "                    'date': str(datetime.now())\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(ds['dir'], RESULTS_FILE), 'r') as f:\n",
    "            ds['results'] = json.loads(f.read())\n",
    "    except FileNotFoundError:\n",
    "        # No results exist\n",
    "        ds['results'] = {}\n",
    "        ds['results']['script'] = ds['meta']['script']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onStepStart(name):\n",
    "    \"\"\"Call this when a new analysis step starts\"\"\"\n",
    "    \n",
    "    logger.info('')\n",
    "    logger.info('#')\n",
    "    logger.info('# ' + name)\n",
    "    logger.info('#')\n",
    "    printMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data sets to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineDataSets():\n",
    "    \n",
    "    global dataSets\n",
    "    \n",
    "    onStepStart('build data sets')\n",
    "    # Define data folders to analyze\n",
    "    dataSets = DATA_SETS\n",
    "\n",
    "    # Read meta data\n",
    "    for ds in dataSets:\n",
    "        buildDataSet(ds)\n",
    "\n",
    "        logger.info(\"Dataset: %s\\nversion: %d (%d), chunks: %.0f, totalLines: %.0f, parseErrors: %.0f, results: %s\" % (\n",
    "            ds['id'],\n",
    "            ds['meta']['script']['version']['major'],\n",
    "            ds['meta']['script']['version']['minor'],\n",
    "            len(ds['meta']['chunks']) if 'chunks' in ds['meta'] else np.NaN,\n",
    "            ds['meta']['totalLines'] if 'totalLines' in ds['meta'] else np.NaN,\n",
    "            len(ds['meta']['httpParseErrors']) if 'httpParseErrors' in ds['meta'] else np.NaN,\n",
    "            str(True if 'results' in ds and len(ds['results']) > 0 else False)\n",
    "        ))\n",
    "    printMemory()\n",
    "defineDataSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsMerge = {\n",
    "    'src': '',\n",
    "    'ipVersion': np.nan,\n",
    "    'id': 'merged'\n",
    "}\n",
    "buildDataSet(dsMerge)\n",
    "printMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "* Deletes any columns that are not required\n",
    "* Sorts scanns by server_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPreparation():\n",
    "    onStepStart('data preparation')\n",
    "    # Prepare data by selecting relevant columns and sort by hostname\n",
    "    for ds in dataSets:\n",
    "        logger.info(\"Preparing %s\" % ds['id'])\n",
    "\n",
    "        # Cut and sort\n",
    "        fileName = buildFileName(ds, prepared=True)\n",
    "        if not isValidFile(fileName, ds['meta'].get('cutSortHash')):\n",
    "            logger.info(\"Cutting and sorting...\")\n",
    "            cutAndSort(ds)\n",
    "            ds['meta']['cutSortHash'] = sha512sum(fileName)\n",
    "\n",
    "        # Save meta data to save progress in this step\n",
    "        saveMeta(ds)\n",
    "        printMemory()\n",
    "\n",
    "dataPreparation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "* Splits data into smaller chunks making sure that all scanns for the same server_name are in the same chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chunking():\n",
    "    onStepStart('chunking')\n",
    "    printMemory()\n",
    "    # Split data into chunks\n",
    "    for ds in dataSets:\n",
    "        logger.debug(\"Splitting data in %s\" % ds['id'])\n",
    "\n",
    "        requiresSplitting = False\n",
    "        splitStart = 0\n",
    "        if 'chunks' in ds['meta']:\n",
    "            # Data has been split before\n",
    "            lastChunkEnd = 0\n",
    "            for nr, chunk in enumerate(ds['meta']['chunks']):\n",
    "                # Check chunk info\n",
    "                if not ('start' in chunk and 'count' in chunk):\n",
    "                    logger.warning(\"Missing info for chunk %d @ %s!\" % (nr, ds['id']))\n",
    "                    requiresSplitting = True\n",
    "                    break\n",
    "\n",
    "                # Check consistency\n",
    "                if lastChunkEnd != chunk['start']:\n",
    "                    logger.warning(\"Inconsistency found at chunk %d @ %s! Expected to start at line %d, but started at line %d\" %\n",
    "                                   (nr, ds['id'], lastChunkEnd, chunk['start']))\n",
    "                    requiresSplitting = True\n",
    "                    splitStart = nr\n",
    "                    break\n",
    "                lastChunkEnd = chunk['start'] + chunk['count']\n",
    "\n",
    "                # Check pickle\n",
    "                if isValidFile(buildFileName(ds, chunk=nr), chunk.get('hash')):\n",
    "                    logger.info(\"Chunk %d @ %s is valid\" % (nr, ds['id']))\n",
    "                    continue\n",
    "\n",
    "                # Re-read chunk\n",
    "                logger.info(\"Chunk %d @ %s is invalid, reading again\" % (nr, ds['id']))\n",
    "                read, relevant = readChunk(ds, nr, chunk['start'], chunk['count'])\n",
    "                if read != chunk['count']:\n",
    "                    logger.warning(\"Read %d lines, expected to read %d lines!\" % (read, chunk['count']))\n",
    "                    requiresSplitting = True\n",
    "                    splitStart = nr\n",
    "                    break\n",
    "\n",
    "                chunk['hash'] = sha512sum(buildFileName(ds, chunk=nr))\n",
    "                chunk['lines'] = relevant\n",
    "\n",
    "            # Check that all lines have been read\n",
    "            if not requiresSplitting:\n",
    "                logger.debug(\"Looking for additional lines\")\n",
    "                read, relevant = readChunk(ds, np.nan, lastChunkEnd, 1)\n",
    "                if read > 0:\n",
    "                    logger.warning(\"Found lines that were not in chunks!\")\n",
    "                    requiresSplitting = True\n",
    "                    # Split only additional data\n",
    "                    splitStart = len(ds['meta']['chunks'])\n",
    "\n",
    "            if requiresSplitting:\n",
    "                # Data has to be re-split, delete old files\n",
    "                for f in os.listdir(ds['dir']):\n",
    "                    match = re.search('^' + HOSTS_FILE + EXT_PREPARED + EXT_CHUNK + '(\\d+).*$', f)\n",
    "                    if match and int(match.group(1)) >= splitStart:\n",
    "                        os.remove(os.path.join(ds['dir'], f))\n",
    "        else:\n",
    "            requiresSplitting = True\n",
    "        printMemory()\n",
    "\n",
    "        # Split chunks\n",
    "        if requiresSplitting:\n",
    "            nr = 0\n",
    "            lastChunkEnd = 0\n",
    "\n",
    "            # Set starting values\n",
    "            if splitStart > 0:\n",
    "                nr = splitStart\n",
    "                prevChunk = ds['meta']['chunks'][splitStart - 1]\n",
    "                lastChunkEnd = prevChunk['start'] + prevChunk['count']\n",
    "                ds['meta']['chunks'] = ds['meta']['chunks'][:splitStart]\n",
    "            else:\n",
    "                ds['meta']['chunks'] = []\n",
    "            printMemory()\n",
    "\n",
    "            while True:\n",
    "                logger.info(\"Building chunk %d @ %s\" % (nr, ds['id']))\n",
    "                printMemory()\n",
    "                chunk = {\n",
    "                    'start': lastChunkEnd,\n",
    "                    'count': CHUNK_SIZE\n",
    "                }\n",
    "\n",
    "                read, relevant = readChunk(ds, nr, chunk['start'], chunk['count'])\n",
    "                logger.debug(\"Read %d lines out of which %d lines are relevant\" % (read, relevant))\n",
    "                chunk['lines'] = relevant\n",
    "                chunk['count'] = read\n",
    "                chunk['hash'] = sha512sum(buildFileName(ds, chunk=nr))\n",
    "\n",
    "                ds['meta']['chunks'].append(chunk)\n",
    "                nr += 1\n",
    "                lastChunkEnd = chunk['start'] + chunk['count']\n",
    "\n",
    "                saveMeta(ds)\n",
    "\n",
    "                if read < CHUNK_SIZE:\n",
    "                    # Last chunk has been read\n",
    "                    break\n",
    "\n",
    "            # Recompute total lines\n",
    "            totalLines = 0\n",
    "            noHttpLines = 0\n",
    "            for chunk in ds['meta']['chunks']:\n",
    "                totalLines += chunk['lines']\n",
    "                noHttpLines += chunk['count'] - chunk['lines']\n",
    "            ds['meta']['totalLines'] = totalLines\n",
    "            ds['results']['totalLines'] = totalLines\n",
    "            ds['results']['noHttpLines'] = noHttpLines\n",
    "\n",
    "        # Save meta data to save progress in this step\n",
    "        saveMeta(ds)\n",
    "        saveResults(ds)\n",
    "        printMemory()\n",
    "\n",
    "chunking()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stsMaxAgePattern = re.compile('^\\s*max-age\\s*=\\s*(?:\"(\\d+)\"|(\\d+))\\s*$')\n",
    "colsToCreate = {\n",
    "    'http-header-parse-error': False,\n",
    "    'http-header--sts': False,\n",
    "    'http-header--sts-max-age': np.NaN,\n",
    "    'http-header--sts-includeSubDomains': False,\n",
    "    'http-header--sts-preload': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseHttpHeaders(headers, errList):\n",
    "    \"\"\"Parses the http headers.\"\"\"\n",
    "    \n",
    "    # Skip empty headers\n",
    "    if type(headers) == float and np.isnan(headers):\n",
    "        return np.NaN\n",
    "        \n",
    "    headers = str(headers)\n",
    "    result = {}\n",
    "    \n",
    "    for header in headers.splitlines():\n",
    "        try:\n",
    "            hName, hValue = header.split(':', 1)\n",
    "        except ValueError:\n",
    "            errList.append(\"Missing header value: '{}'\".format(header))\n",
    "            result['http-header-parse-error'] = True\n",
    "            continue\n",
    "        hNameLower = hName.lower()\n",
    "            \n",
    "        if hNameLower == 'strict-transport-security':\n",
    "            if 'http-header--sts' in result:\n",
    "                # Process only first header (RFC 6797 section 8.1)\n",
    "                errList.append(\"Sts header set more than once\")\n",
    "                result['http-header-parse-error'] = True\n",
    "                continue\n",
    "            \n",
    "            result['http-header--sts'] = True\n",
    "            # Analyze HSTS header\n",
    "            for directive in hValue.split(';'):\n",
    "                directive = directive.strip().lower()\n",
    "                \n",
    "                if directive == '':\n",
    "                    # ignore\n",
    "                    pass\n",
    "                elif directive == 'includesubdomains':\n",
    "                    if 'http-header--sts-includeSubDomains' in result:\n",
    "                        # Directive must only exist once (RFC 6797 section 6.1)\n",
    "                        errList.append(\"Sts directive includeSubDomains set more than once\")\n",
    "                        result['http-header-parse-error'] = True\n",
    "                        result['http-header--sts'] = False\n",
    "                        continue\n",
    "                    result['http-header--sts-includeSubDomains'] = True\n",
    "                elif directive == 'preload':\n",
    "                    if 'http-header--sts-preload' in result:\n",
    "                        # Directive must only exist once (RFC 6797 section 6.1)\n",
    "                        errList.append(\"Sts directive preload set more than once\")\n",
    "                        result['http-header-parse-error'] = True\n",
    "                        result['http-header--sts'] = False\n",
    "                        continue\n",
    "                    result['http-header--sts-preload'] = True\n",
    "                else:\n",
    "                    maResult = stsMaxAgePattern.match(directive)\n",
    "                    if maResult:\n",
    "                        if 'http-header--sts-max-age' in result:\n",
    "                            # Directive must only exist once (RFC 6797 section 6.1)\n",
    "                            errList.append(\"Sts directive max-age set more than once\")\n",
    "                            result['http-header-parse-error'] = True\n",
    "                            result['http-header--sts'] = False\n",
    "                            continue\n",
    "                        result['http-header--sts-max-age'] = int(maResult.group(1) or maResult.group(2))\n",
    "                    else:\n",
    "                        # ignore other elements (RFC 6797 section 6.1)\n",
    "                        errList.append(\"Unknown directive: '{}' in header '{}'\".format(directive, header))\n",
    "                        # This is a parse error, but the header is still valid\n",
    "                        result['http-header-parse-error'] = True\n",
    "            \n",
    "            # Check existence of required columns\n",
    "            if not 'http-header--sts-max-age' in result.keys():\n",
    "                result['http-header--sts'] = False\n",
    "            \n",
    "        else:\n",
    "            # ignore other headers\n",
    "            pass\n",
    "    \n",
    "    if len(result) > 0:\n",
    "        return result\n",
    "    else:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2Cols(df):\n",
    "    \"\"\"Extracts the data in the http dictionary to their columns.\"\"\"\n",
    "    \n",
    "    if type(df['http_header_dict']) == dict:\n",
    "        for key, value in df['http_header_dict'].items():\n",
    "            df[key] = value\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract headers\n",
    "\n",
    "* Analyzes the headers column and extracts sts headers and their configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extractHeaders():    \n",
    "    onStepStart('extract headers')\n",
    "    # Parse http headers\n",
    "    for ds in dataSets:\n",
    "        logger.info(\"Parsing headers in %s\" % ds['id'])\n",
    "        printMemory()\n",
    "        cntParseErrors = collections.Counter()\n",
    "        for nr, chunk in enumerate(ds['meta']['chunks']):\n",
    "            logger.info(\"Processing chunk %d @ %s\" % (nr, ds['id']))\n",
    "            printMemory()\n",
    "            chunkFile = buildFileName(ds, chunk=nr, status='parsed')\n",
    "\n",
    "            if isValidFile(chunkFile, chunk.get('parsed', {}).get('hash', None)):\n",
    "                logger.info(\"Found valid chunk\")\n",
    "                continue\n",
    "\n",
    "            df = loadChunk(ds, nr)\n",
    "            logger.debug(\"Loaded chunk (%d lines)\" % len(df.index))\n",
    "            printMemory()\n",
    "\n",
    "            # Split into a set which has header values and a set which doesn't\n",
    "            dfWithHeaders = df[df['headers'].notna()]\n",
    "            dfWithoutHeaders = df[df['headers'].isna()]\n",
    "            del df\n",
    "            logger.debug(\"Split chunk into set with headers (%d lines) and without headers (%d lines)\" %\n",
    "                         (len(dfWithHeaders.index), len(dfWithoutHeaders.index)))\n",
    "\n",
    "            # Parse headers into dict\n",
    "            parseErrors = []\n",
    "            dfWithHeaders = dfWithHeaders.assign(\n",
    "                http_header_dict = dfWithHeaders['headers']\n",
    "                    .apply(parseHttpHeaders, errList = parseErrors))\n",
    "            logger.debug(\"Parsed headers in %d lines\" % len(dfWithHeaders.index))\n",
    "            printMemory()\n",
    "            \n",
    "            # Add parse errors\n",
    "            logger.debug(\"Found %d parse errors\" % len(parseErrors))\n",
    "            cntParseErrors.update(parseErrors)\n",
    "            del parseErrors\n",
    "            gc.collect()\n",
    "            printMemory()\n",
    "\n",
    "            # Create required columns\n",
    "            for col in colsToCreate.keys():\n",
    "                dfWithHeaders[col] = colsToCreate[col]\n",
    "                dfWithoutHeaders[col] = colsToCreate[col]\n",
    "            logger.debug(\"Created required columns\")\n",
    "            printMemory()\n",
    "\n",
    "            # Extract data from dictionary to columns\n",
    "            dfWithHeaders = dfWithHeaders.apply(lambda x: dict2Cols(x), axis=1)\n",
    "            logger.debug(\"Extracted data from dictionary\")\n",
    "            printMemory()\n",
    "\n",
    "            # Drop dictionary column\n",
    "            dfWithHeaders.drop(['http_header_dict'], 1, inplace=True)\n",
    "            logger.debug(\"Dropped dictionaries\")\n",
    "            printMemory()\n",
    "\n",
    "            # Merge both sets back together\n",
    "            df = dfWithHeaders.append(dfWithoutHeaders)\n",
    "            del dfWithHeaders\n",
    "            del dfWithoutHeaders\n",
    "            logger.debug(\"Merged analyzed headers and empty headers\")\n",
    "            printMemory()\n",
    "\n",
    "            # Drop original headers column\n",
    "            df.drop(['headers'], 1, inplace=True)\n",
    "            logger.debug(\"Dropped raw headers\")\n",
    "\n",
    "            # Sort by server_name (required for merge)\n",
    "            df.sort_values('server_name', axis=0, inplace=True)\n",
    "            logger.debug(\"Sorted values\")\n",
    "\n",
    "            df.to_pickle(chunkFile)\n",
    "            chunk['parsed'] = {\n",
    "                'hash': sha512sum(chunkFile),\n",
    "                'lines': len(df.index)\n",
    "            }\n",
    "            logger.debug(\"Saved analyzed chunk\")\n",
    "\n",
    "            # Free memory\n",
    "            del df\n",
    "            gc.collect()\n",
    "            printMemory()\n",
    "            \n",
    "            saveMeta(ds)\n",
    "\n",
    "        # Analyze parse errors\n",
    "        ds['results']['parseErrors'] = sum(cntParseErrors.values())\n",
    "        logger.info(\"Found %d parse errors.\" % ds['results']['parseErrors'])\n",
    "        saveResults(ds)\n",
    "\n",
    "        for error, occurrences in cntParseErrors.most_common(20):\n",
    "            logger.info(\"%d occurrences of: %s\" % (occurrences, error))\n",
    "\n",
    "        ds['meta']['httpParseErrors'] = [{'occurences': occ, 'error': err} for err, occ in cntParseErrors.most_common()]\n",
    "        saveMeta(ds)\n",
    "\n",
    "        # Remove any data that is not required any more\n",
    "        del cntParseErrors\n",
    "        gc.collect()\n",
    "        printMemory()\n",
    "\n",
    "extractHeaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inconsistency analysis\n",
    "\n",
    "Analyzes inconsitencies between different IPs of the same domain in a single data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existence inconsistency\n",
    "\n",
    "* Analyzes inconsistency in existence of the hsts header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inconsistencyExistence():\n",
    "    onStepStart('existence inconsistency')\n",
    "    for ds in dataSets:\n",
    "        logger.info(\"Processing data in %s\" % ds['id'])\n",
    "\n",
    "        ds['meta']['results'] = ds['meta'].get('results', {})\n",
    "        ds['meta']['results']['chunks'] = ds['meta']['results'].get('chunks', [])\n",
    "\n",
    "        allInconsistent = pd.DataFrame()\n",
    "        for nr, chunk in enumerate(ds['meta']['chunks']):\n",
    "            logger.debug(\"Processing chunk %d @ %s\" % (nr, ds['id']))\n",
    "            printMemory()\n",
    "\n",
    "            df = loadChunk(ds, nr, status='parsed')\n",
    "            printMemory()\n",
    "\n",
    "            # Drop entries where header existence is consistent per domain\n",
    "            withoutConsistent = df.drop_duplicates(subset=['server_name', 'http-header--sts'], keep='first')\n",
    "\n",
    "            # Remaining duplicates are inconsistent\n",
    "            inconsistent = withoutConsistent[withoutConsistent.duplicated(subset='server_name', keep=False)]\n",
    "\n",
    "            # Add all entries for inconsistent domains\n",
    "            inconsistent = df[df['server_name'].isin(inconsistent['server_name'])]\n",
    "\n",
    "            del withoutConsistent\n",
    "            del df\n",
    "            printMemory()\n",
    "\n",
    "            # Add length to meta results\n",
    "            resultsChunk = {}\n",
    "            if len(ds['meta']['results']['chunks']) <= nr:\n",
    "                ds['meta']['results']['chunks'].append(resultsChunk)\n",
    "            else:\n",
    "                resultsChunk = ds['meta']['results']['chunks'][nr]\n",
    "\n",
    "            if not 'inconsistency' in resultsChunk:\n",
    "                resultsChunk['inconsistency'] = {}\n",
    "\n",
    "            resultsChunk['inconsistency']['existence'] = len(inconsistent.index)\n",
    "\n",
    "            logger.info(\"Found %d inconsistent entries\" % ds['meta']['results']['chunks'][nr]['inconsistency']['existence'])\n",
    "\n",
    "            # Merge onto all inconsistent for data set\n",
    "            if allInconsistent.empty:\n",
    "                allInconsistent = inconsistent\n",
    "            else:\n",
    "                allInconsistent = allInconsistent.append(inconsistent)\n",
    "\n",
    "            # Free memory\n",
    "            del inconsistent\n",
    "            gc.collect()\n",
    "\n",
    "            saveMeta(ds)\n",
    "            printMemory()\n",
    "\n",
    "        ds['meta']['results']['inconsistency'] = ds['meta']['results'].get('inconsistency', {})\n",
    "        ds['meta']['results']['inconsistency']['existence'] = {\n",
    "            'lines': len(allInconsistent.index),\n",
    "            'domains': len(allInconsistent.drop_duplicates(subset=['server_name'], keep='first').index)\n",
    "        }\n",
    "        print(\"Found %d inconsistent entries (of %d domains) in data set\" %\n",
    "              (ds['meta']['results']['inconsistency']['existence']['lines'],\n",
    "               ds['meta']['results']['inconsistency']['existence']['domains']))\n",
    "        saveMeta(ds)\n",
    "\n",
    "        # Save inconsistent data\n",
    "        allInconsistent.to_pickle(buildFileName(ds) + EXT_INCONSISTENT + EXT_INCONS_EXISTENCE + EXT_PICKLE)\n",
    "\n",
    "        cols = ['server_name', 'host', 'http-header--sts']\n",
    "        print()\n",
    "        print(\"server_name                              | host                                     | sts\")\n",
    "        print((\"-\" * 41) + \"|\" + (\"-\" * 42) + \"|\" + (\"-\" * 7))\n",
    "        for entry in allInconsistent.as_matrix(cols):\n",
    "            print(\"{:<40} | {:<40} | {:>5}\".format(*entry))\n",
    "        print()\n",
    "\n",
    "        ds['results']['inconsistency'] = ds['results'].get('inconsistency', {})\n",
    "        ds['results']['inconsistency']['existence'] = ds['meta']['results']['inconsistency']['existence']\n",
    "        saveResults(ds)\n",
    "        printMemory()\n",
    "\n",
    "inconsistencyExistence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration inconsistency\n",
    "\n",
    "* Analyzes in consistency in the configuration of the hsts header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inconsistenceConfiguration():\n",
    "    onStepStart('configuration inconsistency')\n",
    "    for ds in dataSets:\n",
    "        logger.info(\"Processing data in %s\" % ds['id'])\n",
    "        ds['meta']['results'] = ds['meta'].get('results', {})\n",
    "        ds['meta']['results']['chunks'] = ds['meta']['results'].get('chunks', [])\n",
    "\n",
    "        allInconsistent = pd.DataFrame()\n",
    "        for nr, chunk in enumerate(ds['meta']['chunks']):\n",
    "            logger.info(\"Processing chunk %d @ %s\" % (nr, ds['id']))\n",
    "            printMemory()\n",
    "\n",
    "            df = loadChunk(ds, nr, status='parsed')\n",
    "            printMemory()\n",
    "\n",
    "            # Drop entries without sts header\n",
    "            df = df[df['http-header--sts']]\n",
    "\n",
    "            # Drop entries where header configuration is consistent per domain\n",
    "            withoutConsistent = df.drop_duplicates(subset=['server_name', 'http-header--sts-max-age',\n",
    "                                                           'http-header--sts-includeSubDomains',\n",
    "                                                           'http-header--sts-preload'], keep='first')\n",
    "\n",
    "            # Remaining duplicates are inconsistent\n",
    "            inconsistent = withoutConsistent[withoutConsistent.duplicated(subset='server_name', keep=False)]\n",
    "\n",
    "            # Add all entries for inconsistent domains\n",
    "            inconsistent = df[df['server_name'].isin(inconsistent['server_name'])]\n",
    "\n",
    "            del withoutConsistent\n",
    "            del df\n",
    "            printMemory()\n",
    "\n",
    "            # Add length to meta results\n",
    "            resultsChunk = {}\n",
    "            if len(ds['meta']['results']['chunks']) <= nr:\n",
    "                ds['meta']['results']['chunks'].append(resultsChunk)\n",
    "            else:\n",
    "                resultsChunk = ds['meta']['results']['chunks'][nr]\n",
    "\n",
    "            if not 'inconsistency' in resultsChunk:\n",
    "                resultsChunk['inconsistency'] = {}\n",
    "\n",
    "            resultsChunk['inconsistency']['configuration'] = len(inconsistent.index)\n",
    "\n",
    "            logger.info(\"Found %d inconsistent entries\" % ds['meta']['results']['chunks'][nr]['inconsistency']['configuration'])\n",
    "\n",
    "            # Merge onto all inconsistent for data set\n",
    "            if allInconsistent.empty:\n",
    "                allInconsistent = inconsistent\n",
    "            else:\n",
    "                allInconsistent = allInconsistent.append(inconsistent)\n",
    "\n",
    "            # Free memory\n",
    "            del inconsistent\n",
    "            gc.collect()\n",
    "\n",
    "            saveMeta(ds)\n",
    "            printMemory()\n",
    "\n",
    "        ds['meta']['results']['inconsistency'] = ds['meta']['results'].get('inconsistency', {})\n",
    "        ds['meta']['results']['inconsistency']['configuration'] = {\n",
    "            'lines': len(allInconsistent.index),\n",
    "            'domains': len(allInconsistent.drop_duplicates(subset=['server_name'], keep='first').index)\n",
    "        }\n",
    "        print(\"Found %d inconsistent entries (of %d domains) in data set\" %\n",
    "              (ds['meta']['results']['inconsistency']['configuration']['lines'],\n",
    "               ds['meta']['results']['inconsistency']['configuration']['domains']))\n",
    "        saveMeta(ds)\n",
    "\n",
    "        # Save inconsistent data\n",
    "        allInconsistent.to_pickle(buildFileName(ds) + EXT_INCONSISTENT + EXT_INCONS_CONFIGURATION + EXT_PICKLE)\n",
    "\n",
    "        cols = ['server_name', 'host', 'http-header--sts-max-age', 'http-header--sts-includeSubDomains', 'http-header--sts-preload']\n",
    "        print()\n",
    "        print(\"server_name                              | host                                     | max-age    | inclSubs | preload\")\n",
    "        print((\"-\" * 41) + \"|\" + (\"-\" * 42) + \"|\" + (\"-\" * 12) + \"|\" + (\"-\" * 10) + \"|\" + (\"-\" * 8))\n",
    "        for entry in allInconsistent.as_matrix(cols):\n",
    "            print(\"{:<40} | {:<40} | {:>10} | {:>8} | {:>7}\".format(*entry))\n",
    "        print()\n",
    "\n",
    "        ds['results']['inconsistency'] = ds['results'].get('inconsistency', {})\n",
    "        ds['results']['inconsistency']['configuration'] = ds['meta']['results']['inconsistency']['configuration']\n",
    "        saveResults(ds)\n",
    "        printMemory()\n",
    "\n",
    "inconsistenceConfiguration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inconsistencyMerge():\n",
    "    # Merge all inconsistent domains\n",
    "    for ds in dataSets:\n",
    "        inconsExist = pd.read_pickle(buildFileName(ds) + EXT_INCONSISTENT + EXT_INCONS_EXISTENCE + EXT_PICKLE)\n",
    "        inconsConfig = pd.read_pickle(buildFileName(ds) + EXT_INCONSISTENT + EXT_INCONS_CONFIGURATION + EXT_PICKLE)\n",
    "\n",
    "        inconsistent = inconsExist.append(inconsConfig, ignore_index=True)\n",
    "        inconsistent.drop_duplicates(subset=['host', 'server_name'], keep='first', inplace=True)\n",
    "\n",
    "        ds['meta']['results']['inconsistency'] = ds['meta']['results'].get('inconsistency', {})\n",
    "        ds['meta']['results']['inconsistency']['total'] = {\n",
    "            'lines': len(inconsistent.index),\n",
    "            'domains': len(inconsistent.drop_duplicates(subset=['server_name'], keep='first').index)\n",
    "        }\n",
    "\n",
    "        saveMeta(ds)\n",
    "\n",
    "        ds['results']['inconsistency'] = ds['results'].get('inconsistency', {})\n",
    "        ds['results']['inconsistency']['total'] = ds['meta']['results']['inconsistency']['total']\n",
    "\n",
    "        saveResults(ds)\n",
    "\n",
    "inconsistencyMerge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPv4 - IPv6 Inconsistency\n",
    "\n",
    "* Looks for inconsistencies between IPv4 and IPv6 scanns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareChunkIPv4v6(df):\n",
    "    \"\"\"Prepares the given chunk for the IPv4/IPv6 inconsistency analysis\"\"\"\n",
    "    \n",
    "    # Required for comparison of sets\n",
    "    df.fillna(value={'http-header--sts-max-age': -1}, inplace=True)\n",
    "\n",
    "    # Build sts configuration (tuple)\n",
    "    df['sts_config'] = list(zip(df['http-header--sts'], df['http-header--sts-max-age'], \\\n",
    "                      df['http-header--sts-includeSubDomains'], df['http-header--sts-preload']))\n",
    "    logger.debug(\"Built sts configuration\")\n",
    "    printMemory()\n",
    "\n",
    "    # Drop not required columns\n",
    "    dfConfig = df[['server_name', 'sts_config']]\n",
    "    del df\n",
    "    gc.collect()\n",
    "    logger.debug(\"Dropped not required columns\")\n",
    "    printMemory()\n",
    "\n",
    "    # Split\n",
    "    dfGroup = dfConfig[dfConfig.duplicated('server_name', keep=False)]\n",
    "    dfNoGroup = dfConfig[~dfConfig.duplicated('server_name', keep=False)]\n",
    "    del dfConfig\n",
    "    gc.collect()\n",
    "    logger.debug(\"Split into those that need grouping (%d) and those that don't (%d)\" % (\n",
    "        len(dfGroup), len(dfNoGroup)\n",
    "    ))\n",
    "    printMemory()\n",
    "\n",
    "    # Group by server_name\n",
    "    dfGroup = dfGroup.groupby(by='server_name', sort=False).aggregate(lambda x: set(x))\n",
    "    dfGroup = dfGroup.reset_index()\n",
    "    logger.debug(\"Grouped by domain\")\n",
    "    printMemory()\n",
    "\n",
    "    # Make non-grouped set\n",
    "    dfNoGroup['sts_config'] = dfNoGroup['sts_config'].apply(lambda x: set([x]))\n",
    "    logger.debug(\"Built sets\")\n",
    "    printMemory()\n",
    "\n",
    "    # Merge grouped and not grouped\n",
    "    df = dfNoGroup.append(dfGroup)\n",
    "    df.sort_values('server_name', inplace=True)\n",
    "    logger.debug(\"Merged grouped and not grouped\")\n",
    "    printMemory()\n",
    "    \n",
    "    # Free memory\n",
    "    del dfNoGroup\n",
    "    del dfGroup\n",
    "    gc.collect()\n",
    "    printMemory()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def inconsistencyIPv4v6():\n",
    "    onStepStart('ipv4/ipv6 inconsistency')\n",
    "    # Iterate over all relevant combinations of chunks\n",
    "    dsToCompare = []\n",
    "    for ds in dataSets:\n",
    "        ds['incons'] = {\n",
    "            'chunk': 0,\n",
    "            'df': pd.DataFrame()\n",
    "        }\n",
    "        dsToCompare.append(ds)\n",
    "\n",
    "    # Compare configurations\n",
    "    inconsistent = pd.DataFrame()\n",
    "    # At least 2 entries required\n",
    "    while(len(dsToCompare) > 1):\n",
    "        logger.info(\"Comparing: \" + str([(ds['id_normal'], ds['incons']['chunk']) for ds in dsToCompare]))\n",
    "        printMemory()\n",
    "\n",
    "        # Load and prepare chunks\n",
    "        for i, ds in enumerate(dsToCompare):\n",
    "            if ds['incons']['df'].empty:\n",
    "                # Load next chunk\n",
    "                df = loadChunk(ds, ds['incons']['chunk'], status='parsed')\n",
    "                logger.debug(\"Loaded chunk %d from %s (%d lines)\" % (ds['incons']['chunk'], ds['id'], len(df.index)))\n",
    "                printMemory()\n",
    "\n",
    "                ds['incons']['df'] = prepareChunkIPv4v6(df)\n",
    "        logger.info(\"Loaded and prepared chunks\")\n",
    "        printMemory()\n",
    "        \n",
    "        # Select first chunk end\n",
    "        chunkEnds = []\n",
    "        for i, ds in enumerate(dsToCompare):\n",
    "            if not ds['incons']['df'].empty:\n",
    "                chunkEnds.append((ds['incons']['df'].iloc[-1]['server_name'], i))\n",
    "        chunkEnds.sort()\n",
    "\n",
    "        splitName, dsWithChunkEnd = chunkEnds[0]\n",
    "        del chunkEnds\n",
    "\n",
    "        # Select merge part from chunks up to splitName\n",
    "        mergeParts = []\n",
    "        for ds in dsToCompare:\n",
    "            df = ds['incons']['df']\n",
    "            splitIndex = np.searchsorted(df['server_name'], splitName)[0] + 1\n",
    "            # Append data up to splitIndex to mergeParts\n",
    "            mergeParts.append((ds['id_normal'], df[:splitIndex]))\n",
    "            # Remove merged part from chunk\n",
    "            ds['incons']['df'] = df[splitIndex:]\n",
    "        printMemory()\n",
    "        \n",
    "        # Merge parts from all chunks\n",
    "        merged = pd.DataFrame()\n",
    "        for dsIdNormal, part in mergeParts:\n",
    "            logger.debug(\"Merging %s to %d entries\" % (dsIdNormal, len(merged.index)))\n",
    "            part.rename(columns={'sts_config': 'sts_config_' + dsIdNormal}, inplace=True)\n",
    "            if merged.empty:\n",
    "                merged = part\n",
    "            else:\n",
    "                merged = merged.merge(part, on='server_name', how='outer', \\\n",
    "                                      suffixes=(None, '_' + dsIdNormal), copy=False)\n",
    "        logger.debug(\"Merged chunks (%d entries)\" % len(merged.index))\n",
    "        printMemory()\n",
    "        \n",
    "        del mergeParts\n",
    "        gc.collect()\n",
    "        printMemory()\n",
    "\n",
    "        configCols = ['sts_config_' + ds['id_normal'] for ds in dsToCompare]\n",
    "\n",
    "        # Drop entries that only appear in a single data set\n",
    "        merged.dropna(axis=0, thresh=2, subset=configCols, inplace=True)\n",
    "        logger.debug(\"Dropped entries that only appear in a single data set (%d entries remaining)\" % len(merged.index))\n",
    "        printMemory()\n",
    "\n",
    "        # Filter inconsistent configurations\n",
    "        inconsistencyQuery = []\n",
    "        for i in range(len(configCols)):\n",
    "            for j in range(i + 1, len(configCols)):\n",
    "                pairCheck = [\n",
    "                    configCols[i] + \" == \" + configCols[i], # i is not nan\n",
    "                    configCols[j] + \" == \" + configCols[j], # j is not nan\n",
    "                    configCols[i] + \" != \" + configCols[j], # if neither is nan then they have to be unequal to be inconsistent\n",
    "                ]\n",
    "                inconsistencyQuery.append(\"(\" + \" and \".join(pairCheck) + \")\")\n",
    "        inconsistencyQuery = \" or \".join(inconsistencyQuery)\n",
    "        logger.debug(\"Inonsistency query: %s\" % inconsistencyQuery)\n",
    "        mergedIncons = merged.query(inconsistencyQuery)\n",
    "        del merged\n",
    "        gc.collect()\n",
    "        logger.debug(\"Filtered inconsistent configurations (%d entries remaining)\" % len(mergedIncons.index))\n",
    "        printMemory()\n",
    "\n",
    "        # Append inconsistent\n",
    "        inconsistent = mergedIncons.append(inconsistent, ignore_index=True)\n",
    "        del mergedIncons\n",
    "        logger.debug(\"Appended inconsistent domains\")\n",
    "        printMemory()\n",
    "\n",
    "        # Determine next chunks\n",
    "        dsToRemove = []\n",
    "        for dsNr, ds in enumerate(dsToCompare):\n",
    "            if ds['incons']['df'].empty:\n",
    "                # End of chunk was reached\n",
    "                if ds['incons']['chunk'] >= len(ds['meta']['chunks']) - 1:\n",
    "                    # Current chunk is last chunk, remove from data sets to compare\n",
    "                    del ds['incons']\n",
    "                    dsToRemove.append(dsNr)\n",
    "                else:\n",
    "                    # Select next chunk\n",
    "                    ds['incons']['chunk'] += 1\n",
    "\n",
    "        # Remove data sets marked for removal\n",
    "        dsToRemove.sort(reverse=True)\n",
    "        for nr in dsToRemove:\n",
    "            del dsToCompare[nr]\n",
    "\n",
    "        # Free memory\n",
    "        del dsToRemove\n",
    "        gc.collect()\n",
    "        printMemory()\n",
    "\n",
    "    # Delete temporary data\n",
    "    for ds in dataSets:\n",
    "        if 'incons' in ds.keys():\n",
    "            del ds['incons']\n",
    "    gc.collect()\n",
    "    printMemory()\n",
    "\n",
    "    if not inconsistent.empty:\n",
    "        # Analyze inconsistencies\n",
    "        inconsistent.drop_duplicates(subset=['server_name'], keep='first', inplace=True)\n",
    "        dsMerge['results']['v4v6inconsistent-domains'] = len(inconsistent.index)\n",
    "        inconsistent.to_pickle(buildFileName(dsMerge) + EXT_INCONSISTENT + EXT_INCONS_V4V6 + EXT_PICKLE)\n",
    "    else:\n",
    "        dsMerge['results']['v4v6inconsistent-domains'] = -1\n",
    "    saveResults(dsMerge)\n",
    "\n",
    "    # Free memory\n",
    "    del inconsistent\n",
    "    gc.collect()\n",
    "    printMemory()\n",
    "\n",
    "    logger.info(\"Found %d inconsistent domains\" % dsMerge['results']['v4v6inconsistent-domains'])\n",
    "\n",
    "inconsistencyIPv4v6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark inconsistent\n",
    "\n",
    "* Marks any lines that are inconsistent in the main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markInconsistent():\n",
    "    onStepStart('mark inconsistent')\n",
    "    for ds in dataSets:\n",
    "        logger.info(\"Processing data in %s\" % ds['id'])\n",
    "\n",
    "        # Load list of all inconsistent server_names\n",
    "        inconsExt = pd.read_pickle(buildFileName(ds) + EXT_INCONSISTENT + EXT_INCONS_EXISTENCE + EXT_PICKLE)\n",
    "        inconsConfig = pd.read_pickle(buildFileName(ds) + EXT_INCONSISTENT + EXT_INCONS_CONFIGURATION + EXT_PICKLE)\n",
    "        printMemory()\n",
    "\n",
    "        # Only one entry per server_name required\n",
    "        inconsExt.drop_duplicates(subset=['server_name'], keep='first', inplace=True)\n",
    "        inconsConfig.drop_duplicates(subset=['server_name'], keep='first', inplace=True)\n",
    "        printMemory()\n",
    "\n",
    "        for nr, chunk in enumerate(ds['meta']['chunks']):\n",
    "            logger.info(\"Processing chunk %d @ %s\" % (nr, ds['id']))\n",
    "\n",
    "            df = loadChunk(ds, nr, status='parsed')\n",
    "            logger.debug(\"Loaded chunk (%d lines)\" % len(df.index))\n",
    "            printMemory()\n",
    "\n",
    "            df['inconsistent-existence'] = df['server_name'].isin(inconsExt['server_name'])\n",
    "            df['inconsistent-configuration'] = df['server_name'].isin(inconsConfig['server_name'])\n",
    "\n",
    "            # Save chunk\n",
    "            df.to_pickle(buildFileName(ds, chunk=nr, status='marked'))\n",
    "\n",
    "            # Free memory\n",
    "            del df\n",
    "            gc.collect()\n",
    "            printMemory()\n",
    "\n",
    "        # Free memory\n",
    "        del inconsExt\n",
    "        del inconsConfig\n",
    "        gc.collect()\n",
    "        printMemory()\n",
    "\n",
    "markInconsistent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce\n",
    "\n",
    "* Reduce the data per data set such that only one entry per domain exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reduce():\n",
    "    onStepStart('reduce')\n",
    "    for ds in dataSets:\n",
    "        logger.info(\"Processing data in %s\" % ds['id'])\n",
    "\n",
    "        reducedCount = 0\n",
    "        for nr, chunk in enumerate(ds['meta']['chunks']):\n",
    "            logger.info(\"Processing chunk %d @ %s\" % (nr, ds['id']))\n",
    "            printMemory()\n",
    "            chunkFile = buildFileName(ds, chunk=nr, status='reduced')\n",
    "\n",
    "            if isValidFile(chunkFile, chunk.get('reduced', {}).get('hash', None)):\n",
    "                logger.info(\"Found valid chunk\")\n",
    "                reducedCount += chunk['reduced']['lines']\n",
    "                continue\n",
    "\n",
    "            df = loadChunk(ds, nr, status='marked')\n",
    "            logger.debug(\"Loaded chunk (%d lines)\" % len(df.index))\n",
    "            printMemory()\n",
    "            \n",
    "            # Remove inconsistent\n",
    "            df = df[~df['inconsistent-existence'] & ~df['inconsistent-configuration']]\n",
    "            logger.debug(\"Dropped inconsistent entries\")\n",
    "            printMemory()\n",
    "\n",
    "            # Count entries per domain\n",
    "            counts = df.groupby(by=['server_name'], sort=False).size().to_frame(name='count').reset_index()\n",
    "\n",
    "            # Drop duplicate entries\n",
    "            df.drop_duplicates(subset=['server_name'], keep='first', inplace=True)\n",
    "            logger.info(\"Reduced chunk (%d lines remaining)\" % len(df.index))\n",
    "            printMemory()\n",
    "\n",
    "            # Add counts for entries\n",
    "            df = df.merge(counts, left_on='server_name', right_on='server_name', how='left')\n",
    "            del counts\n",
    "\n",
    "            # Save reduced chunk\n",
    "            df.to_pickle(chunkFile)\n",
    "            chunk['reduced'] = {\n",
    "                'hash': sha512sum(chunkFile),\n",
    "                'lines': len(df.index)\n",
    "            }\n",
    "            logger.debug(\"Saved reduced chunk\")\n",
    "            printMemory()\n",
    "\n",
    "            reducedCount += chunk['reduced']['lines']\n",
    "            saveMeta(ds)\n",
    "\n",
    "            # Free memory\n",
    "            del df\n",
    "            gc.collect()\n",
    "            printMemory()\n",
    "\n",
    "        ds['results']['domains'] = reducedCount\n",
    "        saveResults(ds)\n",
    "\n",
    "reduce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge\n",
    "\n",
    "Merges all data sets and makes sure that in the merged set only one entry per domain exists.\n",
    "\n",
    "* Selects current chunks out of data sets\n",
    "* Compares chunk ends to figure out which chunk ends earliest\n",
    "* Splits other chunks at that point and merges up to that point\n",
    "* Re-use other part of chunks for next round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge():\n",
    "    onStepStart('merge')\n",
    "\n",
    "    dsMerge['meta']['chunks'] = dsMerge['meta'].get('chunks', [])\n",
    "    dsMerge['meta']['totalLines'] = 0\n",
    "    if(len(dsMerge['meta']['chunks']) > 0):\n",
    "        # Delete old data\n",
    "        for f in os.listdir(dsMerge['dir']):\n",
    "            match = re.search('^' + HOSTS_FILE + EXT_PREPARED + EXT_CHUNK + '\\d+.*$', f)\n",
    "            if match:\n",
    "                os.remove(os.path.join(dsMerge['dir'], f))\n",
    "        dsMerge['meta']['chunks'] = []\n",
    "        dsMerge['meta']['totalLines'] = 0\n",
    "\n",
    "    # Merge data sets\n",
    "    dsToMerge = []\n",
    "    for ds in dataSets:\n",
    "        ds['merge'] = {\n",
    "            'chunk': 0,\n",
    "            'df': pd.DataFrame()\n",
    "        }\n",
    "        dsToMerge.append(ds)\n",
    "\n",
    "    chunksAvailable = True\n",
    "    merged = pd.DataFrame()\n",
    "    while dsToMerge:\n",
    "        logger.info(\"Merging chunks: \" + str([(ds['id'], ds['merge']['chunk']) for ds in dsToMerge]))\n",
    "        printMemory()\n",
    "\n",
    "        # Load chunks\n",
    "        for ds in dsToMerge:\n",
    "            if ds['merge']['df'].empty:\n",
    "                ds['merge']['df'] = loadChunk(ds, ds['merge']['chunk'], status='reduced')\n",
    "        printMemory()\n",
    "\n",
    "        # Select first chunk end\n",
    "        chunkEnds = []\n",
    "        for i, ds in enumerate(dsToMerge):\n",
    "            if not ds['merge']['df'].empty:\n",
    "                chunkEnds.append((ds['merge']['df'].iloc[-1]['server_name'], i))\n",
    "        chunkEnds.sort()\n",
    "\n",
    "        splitName, dsWithChunkEnd = chunkEnds[0]\n",
    "        del chunkEnds\n",
    "\n",
    "        # Merge chunk parts up to splitName\n",
    "        mergeParts = []\n",
    "        for ds in dsToMerge:\n",
    "            df = ds['merge']['df']\n",
    "            splitIndex = np.searchsorted(df['server_name'], splitName)[0] + 1\n",
    "            # Append data up to splitIndex to mergeParts\n",
    "            mergeParts.append((ds['id_normal'], df[:splitIndex]))\n",
    "            # Remove merged part from chunk\n",
    "            ds['merge']['df'] = df[splitIndex:]\n",
    "        printMemory()\n",
    "\n",
    "        # Append all parts together\n",
    "        mergedPart = pd.DataFrame()\n",
    "        for dsName, part in mergeParts:\n",
    "            mergedPart = part.append(mergedPart, ignore_index=True)\n",
    "        mergedPart.drop(columns=['count'], inplace=True)\n",
    "        logger.debug(\"Merged chunks up to '%s' (%d lines)\" % (splitName, len(mergedPart.index)))\n",
    "        printMemory()\n",
    "\n",
    "        # Merge to one entry per server_name    \n",
    "        mergedPart.drop_duplicates(subset=['server_name'], keep='first', inplace=True)\n",
    "        logger.debug(\"Dropped duplicates in merged part (%d lines remaining)\" % len(mergedPart.index))\n",
    "        printMemory()\n",
    "\n",
    "        # Add count columns\n",
    "        for dsName, part in mergeParts:\n",
    "            part = part[['server_name', 'count']].rename(index=str, columns={'count': 'count_' + dsName})\n",
    "            mergedPart = mergedPart.merge(part, left_on='server_name', right_on='server_name', how='left')\n",
    "\n",
    "        # Free memory\n",
    "        del mergeParts\n",
    "        gc.collect()\n",
    "        printMemory()\n",
    "\n",
    "        # Append merged part to merge chunk\n",
    "        if merged.empty:\n",
    "            merged = mergedPart\n",
    "        else:\n",
    "            merged = merged.append(mergedPart, ignore_index=True)\n",
    "        logger.debug(\"Merged chunk now contains %d lines\" % len(merged.index))\n",
    "\n",
    "        # Free memory\n",
    "        del mergedPart\n",
    "        gc.collect()\n",
    "        printMemory()\n",
    "\n",
    "        # Save chunk\n",
    "        while len(merged.index) >= MERGE_CHUNK_SIZE:\n",
    "            chunkNr = len(dsMerge['meta']['chunks'])\n",
    "            chunkFile = buildFileName(dsMerge, chunk=chunkNr, status='reduced')\n",
    "\n",
    "            mergedChunk = merged[:MERGE_CHUNK_SIZE]\n",
    "            mergedChunk.to_pickle(chunkFile)\n",
    "            merged = merged[MERGE_CHUNK_SIZE:]\n",
    "\n",
    "            chunkMeta = {\n",
    "                'hash': sha512sum(chunkFile),\n",
    "                'lines': len(mergedChunk.index)\n",
    "            }\n",
    "            dsMerge['meta']['totalLines'] += chunkMeta['lines']\n",
    "            dsMerge['meta']['chunks'].append(chunkMeta)\n",
    "            saveMeta(dsMerge)\n",
    "\n",
    "            del mergedChunk\n",
    "            del chunkMeta\n",
    "            logger.debug(\"Saved merged chunk %d (%d lines remaining for next chunk)\" % (chunkNr, len(merged.index)))\n",
    "            printMemory()\n",
    "\n",
    "        # Select next chunk in data set with chunk end\n",
    "        dsToDelete = []\n",
    "        for dsNr, ds in enumerate(dsToMerge):\n",
    "            if ds['merge']['df'].empty:\n",
    "                # End of current chunk was reached\n",
    "                if ds['merge']['chunk'] >= len(ds['meta']['chunks']) - 1:\n",
    "                    # Current chunk is last chunk, remove from data sets to merge\n",
    "                    del ds['merge']\n",
    "                    dsToDelete.append(dsNr)\n",
    "                else:\n",
    "                    # Select next chunk\n",
    "                    ds['merge']['chunk'] += 1\n",
    "\n",
    "        # Delete ds marked for deletion\n",
    "        dsToDelete.sort(reverse=True)\n",
    "        for dsNr in dsToDelete:\n",
    "            del dsToMerge[dsNr]\n",
    "        del dsToDelete\n",
    "\n",
    "        # Free memory\n",
    "        gc.collect()\n",
    "        printMemory()\n",
    "\n",
    "    # Save chunk last chunk\n",
    "    while len(merged.index) > 0:\n",
    "        chunkNr = len(dsMerge['meta']['chunks'])\n",
    "        chunkFile = buildFileName(dsMerge, chunk=chunkNr, status='reduced')\n",
    "\n",
    "        mergedChunk = merged[:MERGE_CHUNK_SIZE]\n",
    "        mergedChunk.to_pickle(chunkFile)\n",
    "        merged = merged[MERGE_CHUNK_SIZE:]\n",
    "\n",
    "        chunkMeta = {\n",
    "            'hash': sha512sum(chunkFile),\n",
    "            'lines': len(mergedChunk.index)\n",
    "        }\n",
    "        dsMerge['meta']['totalLines'] += chunkMeta['lines']\n",
    "        dsMerge['meta']['chunks'].append(chunkMeta)\n",
    "        saveMeta(dsMerge)\n",
    "\n",
    "        del mergedChunk\n",
    "        del chunkMeta\n",
    "        logger.debug(\"Saved merged chunk %d (%d lines remaining for next chunk)\" % (chunkNr, len(merged.index)))\n",
    "        printMemory()\n",
    "\n",
    "    saveMeta(dsMerge)\n",
    "\n",
    "    dsMerge['results']['totalLines'] = dsMerge['meta']['totalLines']\n",
    "    dsMerge['results']['domains'] = dsMerge['meta']['totalLines']\n",
    "    saveResults(dsMerge)\n",
    "\n",
    "    # Free memory\n",
    "    del merged\n",
    "    gc.collect()\n",
    "    printMemory()\n",
    "\n",
    "merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze headers\n",
    "\n",
    "* Analyzes the usage of the header and its configuration values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyzeHeaders():\n",
    "    onStepStart('analyze headers')\n",
    "    # Analyze parsed headers\n",
    "    for ds in dataSets + [dsMerge]:\n",
    "        ds['meta']['results'] = ds['meta'].get('results', {})\n",
    "        ds['meta']['results']['chunks'] = ds['meta']['results'].get('chunks', [])\n",
    "\n",
    "        cntMaxAge = collections.Counter()\n",
    "\n",
    "        logger.info(\"Analyzing headers in %s\" % ds['id'])\n",
    "        printMemory()\n",
    "\n",
    "        # Analysis per chunk\n",
    "        for nr, chunk in enumerate(ds['meta']['chunks']):\n",
    "            logger.info(\"Processing chunk %d @ %s\" % (nr, ds['id']))\n",
    "            printMemory()\n",
    "\n",
    "            results = {}\n",
    "            if len(ds['meta']['results']['chunks']) <= nr:\n",
    "                ds['meta']['results']['chunks'].append(results)\n",
    "            else:\n",
    "                results = ds['meta']['results']['chunks'][nr]\n",
    "\n",
    "            # Load reduced chunk\n",
    "            df = loadChunk(ds, nr, 'reduced')\n",
    "            printMemory()\n",
    "\n",
    "            # Analyze parse error rate\n",
    "            results['parse-error'] = len(df[df['http-header-parse-error']].index)\n",
    "            logger.info(\"%d have a parse error\" % results['parse-error'])\n",
    "\n",
    "            # General sts usage\n",
    "            df = df[df['http-header--sts']]\n",
    "            results['sts'] = len(df.index)\n",
    "            logger.info(\"%d use sts\" % results['sts'])\n",
    "\n",
    "            # max-age\n",
    "            cntMaxAge.update(df['http-header--sts-max-age'].tolist())\n",
    "\n",
    "            # includeSubDomains\n",
    "            results['sts-includeSubDomains'] = int(df[df['http-header--sts-includeSubDomains']]['http-header--sts-includeSubDomains'].count())\n",
    "            logger.info(\"%d include sub domains\" % results['sts-includeSubDomains'])\n",
    "\n",
    "            # preload\n",
    "            results['sts-preload'] = int(df[df['http-header--sts-preload']]['http-header--sts-preload'].count())\n",
    "            logger.info(\"%d use preload\" % results['sts-preload'])\n",
    "\n",
    "            # Free memory\n",
    "            del df\n",
    "            gc.collect()\n",
    "            printMemory()\n",
    "\n",
    "            saveMeta(ds)\n",
    "\n",
    "        # Sum results\n",
    "        resultSums = {key: 0 for key in ['parse-error', 'sts', 'sts-includeSubDomains', 'sts-preload']}\n",
    "        for chunkResults in ds['meta']['results']['chunks']:\n",
    "            for key in resultSums.keys():\n",
    "                resultSums[key] += chunkResults.get(key, 0)\n",
    "        for key, value in resultSums.items():\n",
    "            ds['meta']['results'][key] = value\n",
    "\n",
    "        ds['meta']['results']['sts-max-age'] = [{'occurences': occ, 'value': val} for val, occ in cntMaxAge.most_common()]\n",
    "        saveMeta(ds)\n",
    "        printMemory()\n",
    "\n",
    "        # Copy end results\n",
    "        for key in resultSums.keys():\n",
    "            ds['results'][key] = ds['meta']['results'][key]\n",
    "\n",
    "        # Aggregate max age values\n",
    "        ds['results']['sts-max-age'] = {aggKey: 0 for aggKey in maxAgeAggregation}\n",
    "        maxAgeAggValues = list(maxAgeAggregation.items())\n",
    "        maxAgeAggValues.sort(key=lambda x: x[1])\n",
    "        for val, occ in cntMaxAge.most_common():\n",
    "            for aggKey, aggVal in maxAgeAggValues:\n",
    "                if val <= aggVal:\n",
    "                    ds['results']['sts-max-age'][aggKey] += occ\n",
    "                    break\n",
    "        saveResults(ds)\n",
    "        printMemory()\n",
    "\n",
    "analyzeHeaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResults():\n",
    "    for ds in dataSets + [dsMerge]:\n",
    "        results = ds.get('results', {})\n",
    "        logger.info(\"Results for %s\" % ds['id'])\n",
    "\n",
    "        logger.info(\"%d scanned domains\" % results['domains'])\n",
    "        if results['domains'] == 0:\n",
    "            continue\n",
    "        logger.info(\"%d (%.4f%% of all scanned domains) use sts\" % (\n",
    "            results['sts'],\n",
    "            results['sts'] / results['domains'] * 100.0\n",
    "        ))\n",
    "        if results['sts'] == 0:\n",
    "            continue\n",
    "        logger.info(\"%d (%.4f%% of all scanned domains) had a parse error\" % (\n",
    "            results['parse-error'],\n",
    "            results['parse-error'] / results['domains'] * 100.0\n",
    "        ))\n",
    "        logger.info(\"%d (%.4f%% of domains with a valid sts header) include the subdomains (%.4f%% of all domains)\" % (\n",
    "            results['sts-includeSubDomains'],\n",
    "            results['sts-includeSubDomains'] / results['sts'] * 100.0,\n",
    "            results['sts-includeSubDomains'] / results['domains'] * 100.0\n",
    "        ))\n",
    "        logger.info(\"%d (%.4f%% of hosts with a valid sts header) preload the domain (%.4f%% of all domains)\" % (\n",
    "            results['sts-preload'],\n",
    "            results['sts-preload'] / results['sts'] * 100.0,\n",
    "            results['sts-preload'] / results['domains'] * 100.0\n",
    "        ))\n",
    "\n",
    "printResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate HSTS Domain list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genHstsDomainList():\n",
    "    onStepStart('hsts domain list')\n",
    "    for ds in dataSets + [dsMerge]:\n",
    "        logger.info(\"Processing %s\" % ds['id'])\n",
    "        sts = pd.DataFrame()\n",
    "        for nr, chunk in enumerate(ds['meta']['chunks']):\n",
    "            logger.info(\"Loading chunk %d\" % nr)\n",
    "            df = loadChunk(ds, nr, status='reduced')\n",
    "\n",
    "            # Only consistent\n",
    "            df = df[~df['inconsistent-existence']]\n",
    "            df = df[~df['inconsistent-configuration']]\n",
    "\n",
    "            # Filter sts domains\n",
    "            df = df[df['http-header--sts']]\n",
    "\n",
    "            # Append domains\n",
    "            sts = df.append(sts, ignore_index=True)\n",
    "\n",
    "            del df\n",
    "\n",
    "        # Just to be sure\n",
    "        sts.drop_duplicates('server_name', keep='first', inplace=True)\n",
    "        sts.sort_values('server_name', inplace=True)\n",
    "\n",
    "        # Save domain list\n",
    "        sts['server_name'].to_csv(buildFileName(ds) + '.consistent.hsts-domains.csv', index=False)\n",
    "\n",
    "        logger.info(\"Saved consistent hsts domains\")\n",
    "\n",
    "        # Free memory\n",
    "        del sts\n",
    "        gc.collect()\n",
    "\n",
    "#genHstsDomainList()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
